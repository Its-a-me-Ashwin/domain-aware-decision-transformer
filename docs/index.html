<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Learning Domain-Adaptive Manipulation Sequences with
    Environment Models and Decision Transformers</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <div class="container">
      <h1>Learning Domain-Adaptive Manipulation Sequences with
        Environment Models and Decision Transformers</h1>
      <p>by Ashwin Ravindra Bharadwaj at Northeastern University</p>
      <nav>
        <ul>
          <li><a href="#abstract">Abstract</a></li>
          <li><a href="#importance">Importance</a></li>
          <li><a href="#architecture">Architecture</a></li>
          <li><a href="#results">Results</a></li>
          <li><a href="#code">Code</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="container">
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        We propose a novel approach for solving goal-conditioned manipulation tasks by leveraging a domain-
adaptive Decision Transformer. Our model predicts sequences of actions that transition an environment
from an initial to a final state, conditioned on both visual and semantic features. We train this system
using synthetic data generated via domain-randomized physics environments and test its generalization
on new unseen settings. Experimental results demonstrate strong performance across multiple domain
shifts.
      </p>
    </section>

    <section id="importance">
      <h2>Why This Work Matters</h2>
      <p>
        Robotic manipulation in dynamic and varied environments remains a key challenge in embodied AI. While
recent progress in transformers and representation learning has enabled long-horizon planning, most models
fail under domain shift due to rigid assumptions about the environment’s structure. In this work, we
propose a domain-adaptive Decision Transformer trained on procedurally generated manipulation data and
demonstrate its robustness across variations in physics, friction, gravity, and object composition.
      </p>
    </section>

    <section id="architecture">
      <h2>Model Architecture</h2>
      <img src="assets/images/model_architecture.png" alt="Model Architecture Diagram" />
      <p>
        he model processes these sequences autoregressively, predicting future actions, states, and returns-to-
go. We introduce normalization layers between transformer blocks to stabilize training across modalities,
following strategies shown effective in multi-modal transformer architectures.
      </p>
    </section>

    <section id="results">
      <h2>Results</h2>
      <div class="results-images">
        <img src="assets/graphs/reward_curve.png" alt="Training Reward Curve" />
        <img src="assets/graphs/generalization_test.png" alt="Generalization Test Results" />
      </div>
      <div class="results-video">
        <video controls>
          <source src="assets/videos/demo_episode.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Watch the robot navigate in real-time, adapting to changing environments.</p>
      </div>
    </section>

    <section id="code">
      <h2>Source Code</h2>
      <p>Explore our entire codebase, training scripts, and detailed experiments in this repository. Feel free to dive in and experiment.</p>
      <a class="repo-link" href="https://github.com/your-username/your-repo" target="_blank">Visit our GitHub Repository</a>
    </section>
  </main>

  <footer>
    <div class="container">
      <p>© 2025 [Your Name or Team]. All rights reserved.</p>
    </div>
  </footer>

  <script src="script.js"></script>
</body>
</html>
